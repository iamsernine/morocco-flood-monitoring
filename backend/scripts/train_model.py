"""
============================================================================
TRAIN_MODEL.PY - Script d'entra√Ænement du mod√®le ML
============================================================================
Description:
    Entra√Æne un mod√®le de pr√©diction d'inondation bas√© sur des donn√©es
    historiques. Utilise RandomForestClassifier pour la classification
    binaire (inondation / pas d'inondation).

Fonctionnalit√©s:
    - G√©n√©ration de donn√©es synth√©tiques pour d√©monstration
    - Entra√Ænement du mod√®le avec validation crois√©e
    - Sauvegarde du mod√®le et du scaler
    - √âvaluation des performances

Usage:
    python scripts/train_model.py

Debugging:
    - V√©rifier que scikit-learn est install√©
    - V√©rifier que le dossier ml_models/ existe
    - Inspecter les m√©triques de performance
============================================================================
"""

import os
import sys
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import joblib

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def generate_synthetic_data(n_samples=10000):
    """
    G√©n√®re des donn√©es synth√©tiques pour l'entra√Ænement.
    
    En production, ces donn√©es seraient remplac√©es par des donn√©es
    historiques r√©elles collect√©es par les capteurs.
    
    Args:
        n_samples: Nombre d'√©chantillons √† g√©n√©rer
    
    Returns:
        DataFrame avec features et target
    """
    print(f"G√©n√©ration de {n_samples} √©chantillons synth√©tiques...")
    
    np.random.seed(42)
    
    # Features
    data = {
        'water_level_avg': np.random.uniform(0, 100, n_samples),
        'water_level_max': np.random.uniform(0, 100, n_samples),
        'water_level_slope': np.random.uniform(-5, 5, n_samples),
        'humidity_avg': np.random.uniform(0, 100, n_samples),
        'humidity_max': np.random.uniform(0, 100, n_samples),
        'humidity_slope': np.random.uniform(-2, 2, n_samples),
        'rainfall': np.random.uniform(0, 100, n_samples),
        'temperature': np.random.uniform(5, 45, n_samples),
        'wind_speed': np.random.uniform(0, 50, n_samples),
        'river_level': np.random.uniform(0, 100, n_samples),
        'soil_moisture': np.random.uniform(0, 100, n_samples),
    }
    
    df = pd.DataFrame(data)
    
    # Logique de g√©n√©ration du target (inondation)
    # R√®gles simplifi√©es pour la d√©monstration
    flood = (
        (df['water_level_avg'] > 70) |
        (df['rainfall'] > 40) |
        ((df['water_level_avg'] > 50) & (df['rainfall'] > 25)) |
        ((df['river_level'] > 70) & (df['soil_moisture'] > 80)) |
        ((df['water_level_slope'] > 2) & (df['water_level_avg'] > 40))
    ).astype(int)
    
    df['flood'] = flood
    
    # Ajouter du bruit pour rendre les donn√©es plus r√©alistes
    noise_indices = np.random.choice(len(df), size=int(len(df) * 0.1), replace=False)
    df.loc[noise_indices, 'flood'] = 1 - df.loc[noise_indices, 'flood']
    
    print(f"‚úÖ Donn√©es g√©n√©r√©es: {len(df)} √©chantillons")
    print(f"   - Inondations: {df['flood'].sum()} ({df['flood'].mean()*100:.1f}%)")
    print(f"   - Pas d'inondation: {(1-df['flood']).sum()} ({(1-df['flood']).mean()*100:.1f}%)")
    
    return df


def train_model(df):
    """
    Entra√Æne le mod√®le de pr√©diction.
    
    Args:
        df: DataFrame avec features et target
    
    Returns:
        Tuple (model, scaler, metrics)
    """
    print("\nüîß Entra√Ænement du mod√®le...")
    
    # S√©parer features et target
    feature_cols = [
        'water_level_avg', 'water_level_max', 'water_level_slope',
        'humidity_avg', 'humidity_max', 'humidity_slope',
        'rainfall', 'temperature', 'wind_speed',
        'river_level', 'soil_moisture'
    ]
    
    X = df[feature_cols]
    y = df['flood']
    
    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"   - Train: {len(X_train)} √©chantillons")
    print(f"   - Test: {len(X_test)} √©chantillons")
    
    # Normalisation
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Entra√Ænement
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train_scaled, y_train)
    
    # Validation crois√©e
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')
    print(f"\nüìä Validation crois√©e (ROC-AUC): {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
    
    # √âvaluation sur le test set
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    
    print("\nüìà Performances sur le test set:")
    print(classification_report(y_test, y_pred, target_names=['Pas d\'inondation', 'Inondation']))
    
    print("\nMatrice de confusion:")
    print(confusion_matrix(y_test, y_pred))
    
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    print(f"\nROC-AUC Score: {roc_auc:.4f}")
    
    # Feature importance
    print("\nüîç Importance des features:")
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    for idx, row in feature_importance.iterrows():
        print(f"   {row['feature']:25s}: {row['importance']:.4f}")
    
    metrics = {
        'cv_roc_auc': cv_scores.mean(),
        'test_roc_auc': roc_auc,
        'feature_importance': feature_importance.to_dict('records')
    }
    
    return model, scaler, metrics


def save_model(model, scaler, metrics):
    """
    Sauvegarde le mod√®le et le scaler.
    
    Args:
        model: Mod√®le entra√Æn√©
        scaler: Scaler pour normalisation
        metrics: M√©triques de performance
    """
    print("\nüíæ Sauvegarde du mod√®le...")
    
    # Cr√©er le dossier ml_models
    model_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        'ml_models'
    )
    os.makedirs(model_dir, exist_ok=True)
    
    # Sauvegarder le mod√®le
    model_path = os.path.join(model_dir, 'flood_model.pkl')
    
    model_data = {
        'classifier': model,
        'scaler': scaler,
        'metrics': metrics,
        'is_default': False
    }
    
    joblib.dump(model_data, model_path)
    
    print(f"‚úÖ Mod√®le sauvegard√©: {model_path}")
    print(f"   - Taille: {os.path.getsize(model_path) / 1024:.2f} KB")


def main():
    """Fonction principale."""
    print("=" * 80)
    print("üåä ENTRA√éNEMENT DU MOD√àLE DE PR√âDICTION D'INONDATION")
    print("=" * 80)
    
    # G√©n√©rer des donn√©es synth√©tiques
    df = generate_synthetic_data(n_samples=10000)
    
    # Entra√Æner le mod√®le
    model, scaler, metrics = train_model(df)
    
    # Sauvegarder le mod√®le
    save_model(model, scaler, metrics)
    
    print("\n" + "=" * 80)
    print("‚úÖ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS")
    print("=" * 80)
    print("\nLe mod√®le est pr√™t √† √™tre utilis√© par le service de pr√©diction.")
    print("Red√©marrez le backend pour charger le nouveau mod√®le.")


if __name__ == '__main__':
    main()
