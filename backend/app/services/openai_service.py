"""
============================================================================
OPENAI_SERVICE.PY - Service d'explications IA via OpenAI
============================================================================
Description:
    Service qui utilise l'API OpenAI pour g√©n√©rer des explications en
    langage naturel des pr√©dictions d'inondation et cr√©er des rapports.

Fonctionnalit√©s:
    - G√©n√©ration d'explications pour les pr√©dictions
    - Cr√©ation de rapports personnalis√©s
    - Optimisation des tokens pour minimiser les co√ªts
    - Support multilingue (fran√ßais par d√©faut)

Usage:
    from app.services.openai_service import OpenAIService
    
    service = OpenAIService()
    explanation = service.generate_explanation(prediction_data)
    report = service.generate_report(cities, sensors, metrics, time_range)

Debugging:
    - V√©rifier que la cl√© API OpenAI est configur√©e
    - Surveiller l'utilisation des tokens
    - Tester avec des donn√©es simul√©es
    - V√©rifier les logs d'erreur API
============================================================================
"""

import os
from typing import Dict, Any, List, Optional
from datetime import datetime
from openai import OpenAI

from app.services.config_service import get_config_service


class OpenAIService:
    """
    Service d'explications IA via OpenAI.
    """
    
    def __init__(self):
        """Initialise le service OpenAI."""
        self.config = get_config_service()
        
        # R√©cup√©rer la cl√© API
        api_key = self.config.get('openai_api_key', '')
        
        if not api_key or api_key.strip() == '':
            print("‚ö†Ô∏è  Cl√© API OpenAI non configur√©e")
            self.client = None
        else:
            self.client = OpenAI(api_key=api_key)
            print("‚úÖ Service OpenAI initialis√©")
        
        # Configuration par d√©faut
        self.model = "gpt-4.1-mini"  # Mod√®le optimis√© co√ªt/performance
        self.max_tokens = 500  # Limite pour minimiser les co√ªts
        self.temperature = 0.7
    
    def is_available(self) -> bool:
        """
        V√©rifie si le service OpenAI est disponible.
        
        Returns:
            True si la cl√© API est configur√©e, False sinon
        """
        return self.client is not None
    
    def generate_explanation(self, prediction_data: Dict[str, Any], 
                           language: str = 'fr') -> Optional[str]:
        """
        G√©n√®re une explication en langage naturel pour une pr√©diction.
        
        Args:
            prediction_data: Donn√©es de pr√©diction (probability, risk_level, input_data)
            language: Langue de l'explication ('fr' ou 'en')
        
        Returns:
            Texte d'explication ou None si erreur
        """
        if not self.is_available():
            return self._generate_fallback_explanation(prediction_data, language)
        
        try:
            # Construire le prompt
            prompt = self._build_explanation_prompt(prediction_data, language)
            
            # Appel API OpenAI
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": self._get_system_prompt(language)
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature,
            )
            
            explanation = response.choices[0].message.content.strip()
            
            # Log de l'utilisation des tokens
            tokens_used = response.usage.total_tokens
            print(f"üìä Tokens utilis√©s: {tokens_used}")
            
            return explanation
        
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de l'explication: {e}")
            return self._generate_fallback_explanation(prediction_data, language)
    
    def _get_system_prompt(self, language: str) -> str:
        """
        Retourne le prompt syst√®me selon la langue.
        
        Args:
            language: Langue ('fr' ou 'en')
        
        Returns:
            Prompt syst√®me
        """
        if language == 'fr':
            return """Tu es un expert en hydrologie et gestion des risques d'inondation au Maroc.
            Tu dois expliquer les pr√©dictions d'inondation de mani√®re claire et concise pour 
            des gestionnaires municipaux. Utilise un langage professionnel mais accessible.
            Limite tes r√©ponses √† 2-3 phrases maximum."""
        else:
            return """You are an expert in hydrology and flood risk management in Morocco.
            You must explain flood predictions clearly and concisely for municipal managers.
            Use professional but accessible language. Limit your responses to 2-3 sentences maximum."""
    
    def _build_explanation_prompt(self, prediction_data: Dict[str, Any], 
                                 language: str) -> str:
        """
        Construit le prompt pour l'explication.
        
        Args:
            prediction_data: Donn√©es de pr√©diction
            language: Langue
        
        Returns:
            Prompt format√©
        """
        sensor_id = prediction_data.get('sensor_id', 'N/A')
        city = prediction_data.get('city_name', 'N/A')
        probability = prediction_data.get('probability', 0)
        risk_level = prediction_data.get('risk_level', 'Low')
        
        input_data = prediction_data.get('input_data', {})
        water_level = input_data.get('water_level_avg', 0)
        rainfall = input_data.get('rainfall', 0)
        river_level = input_data.get('river_level', 0)
        
        if language == 'fr':
            prompt = f"""Explique cette pr√©diction d'inondation:
            
Ville: {city}
Capteur: {sensor_id}
Probabilit√© d'inondation: {probability}%
Niveau de risque: {risk_level}

Donn√©es cl√©s:
- Niveau d'eau: {water_level} cm
- Pr√©cipitations: {rainfall} mm
- Niveau de la rivi√®re: {river_level} cm

Fournis une explication concise (2-3 phrases) des facteurs de risque et recommandations."""
        else:
            prompt = f"""Explain this flood prediction:
            
City: {city}
Sensor: {sensor_id}
Flood probability: {probability}%
Risk level: {risk_level}

Key data:
- Water level: {water_level} cm
- Rainfall: {rainfall} mm
- River level: {river_level} cm

Provide a concise explanation (2-3 sentences) of risk factors and recommendations."""
        
        return prompt
    
    def _generate_fallback_explanation(self, prediction_data: Dict[str, Any], 
                                      language: str) -> str:
        """
        G√©n√®re une explication simple sans OpenAI (fallback).
        
        Args:
            prediction_data: Donn√©es de pr√©diction
            language: Langue
        
        Returns:
            Explication g√©n√©r√©e
        """
        probability = prediction_data.get('probability', 0)
        risk_level = prediction_data.get('risk_level', 'Low')
        
        if language == 'fr':
            if risk_level == 'High':
                return f"Risque √©lev√© d'inondation d√©tect√© ({probability}%). Surveillance accrue recommand√©e et activation des mesures pr√©ventives."
            elif risk_level == 'Medium':
                return f"Risque mod√©r√© d'inondation ({probability}%). Maintenir la surveillance et pr√©parer les √©quipes d'intervention."
            else:
                return f"Risque faible d'inondation ({probability}%). Situation normale, surveillance de routine."
        else:
            if risk_level == 'High':
                return f"High flood risk detected ({probability}%). Increased monitoring and preventive measures recommended."
            elif risk_level == 'Medium':
                return f"Moderate flood risk ({probability}%). Maintain monitoring and prepare response teams."
            else:
                return f"Low flood risk ({probability}%). Normal situation, routine monitoring."
    
    def generate_report(self, cities: List[str], sensors: List[str], 
                       metrics: List[str], time_range: str,
                       summary_data: Dict[str, Any],
                       language: str = 'fr') -> Optional[str]:
        """
        G√©n√®re un rapport personnalis√©.
        
        Args:
            cities: Liste des villes √† inclure
            sensors: Liste des capteurs √† inclure
            metrics: Liste des m√©triques √† analyser
            time_range: P√©riode du rapport
            summary_data: Donn√©es de synth√®se
            language: Langue du rapport
        
        Returns:
            Texte du rapport ou None si erreur
        """
        if not self.is_available():
            return self._generate_fallback_report(cities, sensors, time_range, summary_data, language)
        
        try:
            # Construire le prompt
            prompt = self._build_report_prompt(cities, sensors, metrics, time_range, summary_data, language)
            
            # Appel API OpenAI
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": self._get_report_system_prompt(language)
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=1000,  # Plus de tokens pour un rapport complet
                temperature=0.7,
            )
            
            report = response.choices[0].message.content.strip()
            
            # Log de l'utilisation des tokens
            tokens_used = response.usage.total_tokens
            print(f"üìä Tokens utilis√©s pour le rapport: {tokens_used}")
            
            return report
        
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration du rapport: {e}")
            return self._generate_fallback_report(cities, sensors, time_range, summary_data, language)
    
    def _get_report_system_prompt(self, language: str) -> str:
        """Retourne le prompt syst√®me pour les rapports."""
        if language == 'fr':
            return """Tu es un expert en analyse de donn√©es hydrologiques et gestion des risques.
            Tu dois cr√©er des rapports professionnels pour les autorit√©s municipales marocaines.
            Structure le rapport avec des sections claires: R√©sum√©, Analyse, Recommandations."""
        else:
            return """You are an expert in hydrological data analysis and risk management.
            You must create professional reports for Moroccan municipal authorities.
            Structure the report with clear sections: Summary, Analysis, Recommendations."""
    
    def _build_report_prompt(self, cities: List[str], sensors: List[str],
                            metrics: List[str], time_range: str,
                            summary_data: Dict[str, Any], language: str) -> str:
        """Construit le prompt pour le rapport."""
        if language == 'fr':
            prompt = f"""G√©n√®re un rapport de surveillance des inondations:

P√©riode: {time_range}
Villes: {', '.join(cities)}
Nombre de capteurs: {len(sensors)}
M√©triques analys√©es: {', '.join(metrics)}

Donn√©es de synth√®se:
{self._format_summary_data(summary_data, language)}

Cr√©e un rapport structur√© avec:
1. R√©sum√© ex√©cutif
2. Analyse des risques par ville
3. Recommandations d'action"""
        else:
            prompt = f"""Generate a flood monitoring report:

Period: {time_range}
Cities: {', '.join(cities)}
Number of sensors: {len(sensors)}
Analyzed metrics: {', '.join(metrics)}

Summary data:
{self._format_summary_data(summary_data, language)}

Create a structured report with:
1. Executive summary
2. Risk analysis by city
3. Action recommendations"""
        
        return prompt
    
    def _format_summary_data(self, summary_data: Dict[str, Any], language: str) -> str:
        """Formate les donn√©es de synth√®se pour le prompt."""
        formatted = []
        for city, data in summary_data.items():
            if language == 'fr':
                formatted.append(f"- {city}: {data.get('total_sensors', 0)} capteurs, "
                               f"risque moyen {data.get('avg_probability', 0)}%, "
                               f"{data.get('high_risk', 0)} alertes √©lev√©es")
            else:
                formatted.append(f"- {city}: {data.get('total_sensors', 0)} sensors, "
                               f"average risk {data.get('avg_probability', 0)}%, "
                               f"{data.get('high_risk', 0)} high alerts")
        return '\n'.join(formatted)
    
    def _generate_fallback_report(self, cities: List[str], sensors: List[str],
                                 time_range: str, summary_data: Dict[str, Any],
                                 language: str) -> str:
        """G√©n√®re un rapport simple sans OpenAI."""
        if language == 'fr':
            report = f"""RAPPORT DE SURVEILLANCE DES INONDATIONS
P√©riode: {time_range}
G√©n√©r√© le: {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}

R√âSUM√â
Villes surveill√©es: {', '.join(cities)}
Nombre de capteurs: {len(sensors)}

ANALYSE PAR VILLE
"""
            for city, data in summary_data.items():
                report += f"\n{city}:\n"
                report += f"  - Capteurs actifs: {data.get('total_sensors', 0)}\n"
                report += f"  - Probabilit√© moyenne: {data.get('avg_probability', 0)}%\n"
                report += f"  - Alertes √©lev√©es: {data.get('high_risk', 0)}\n"
            
            report += "\nRECOMMANDATIONS\n"
            report += "- Maintenir la surveillance continue\n"
            report += "- V√©rifier l'√©tat des √©quipements\n"
            report += "- Pr√©parer les √©quipes d'intervention\n"
        else:
            report = f"""FLOOD MONITORING REPORT
Period: {time_range}
Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}

SUMMARY
Monitored cities: {', '.join(cities)}
Number of sensors: {len(sensors)}

ANALYSIS BY CITY
"""
            for city, data in summary_data.items():
                report += f"\n{city}:\n"
                report += f"  - Active sensors: {data.get('total_sensors', 0)}\n"
                report += f"  - Average probability: {data.get('avg_probability', 0)}%\n"
                report += f"  - High alerts: {data.get('high_risk', 0)}\n"
            
            report += "\nRECOMMENDATIONS\n"
            report += "- Maintain continuous monitoring\n"
            report += "- Check equipment status\n"
            report += "- Prepare response teams\n"
        
        return report


# ============================================================================
# TESTS
# ============================================================================

if __name__ == "__main__":
    print("Test du OpenAIService...")
    service = OpenAIService()
    
    if service.is_available():
        # Test explication
        test_prediction = {
            'sensor_id': 'CAS_1',
            'city_name': 'Casablanca',
            'probability': 75.5,
            'risk_level': 'High',
            'input_data': {
                'water_level_avg': 80.0,
                'rainfall': 35.0,
                'river_level': 70.0,
            }
        }
        
        explanation = service.generate_explanation(test_prediction, 'fr')
        print(f"\nExplication g√©n√©r√©e:\n{explanation}")
    else:
        print("‚ö†Ô∏è  Service OpenAI non disponible (cl√© API manquante)")
        
        # Test fallback
        test_prediction = {
            'probability': 75.5,
            'risk_level': 'High',
        }
        explanation = service._generate_fallback_explanation(test_prediction, 'fr')
        print(f"\nExplication fallback:\n{explanation}")
    
    print("\n‚úÖ Tests termin√©s!")
